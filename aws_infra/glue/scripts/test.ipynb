{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "222f6f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi, nice to meet you\n"
     ]
    }
   ],
   "source": [
    "print('Hi, nice to meet you')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa45caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "inputDF = glueContext.create_dynamic_frame_from_options(connection_type=\"s3\",\n",
    "                                                        connection_options = {\"paths\": [\"s3://ti-dev-terraform-12345/raw/orders/orders.csv\"]}, format = \"csv\")\n",
    "inputDF.show(5)\n",
    "df = inputDF.toDF()\n",
    "df_pd = df.toPandas()\n",
    "print('test script has successfully ran')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24463cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"col0\": \"salesorderid\", \"col1\": \"salesorderdetailid\", \"col2\": \"orderdate\", \"col3\": \"duedate\", \"col4\": \"shipdate\", \"col5\": \"employeeid\", \"col6\": \"customerid\", \"col7\": \"subtotal\", \"col8\": \"taxamt\", \"col9\": \"freight\", \"col10\": \"totaldue\", \"col11\": \"productid\", \"col12\": \"orderqty\", \"col13\": \"unitprice\", \"col14\": \"unitpricediscount\", \"col15\": \"linetotal\"}\n",
      "{\"col0\": \"43659\", \"col1\": \"1\", \"col2\": \"5/31/2011\", \"col3\": \"6/12/2011\", \"col4\": \"6/7/2011\", \"col5\": \"279\", \"col6\": \"1045\", \"col7\": \"20565.6206\", \"col8\": \"1971.5149\", \"col9\": \"616.0984\", \"col10\": \"23153.2339\", \"col11\": \"776\", \"col12\": \"1\", \"col13\": \"2024.994\", \"col14\": \"0\", \"col15\": \"2024.994\"}\n",
      "{\"col0\": \"43659\", \"col1\": \"2\", \"col2\": \"5/31/2011\", \"col3\": \"6/12/2011\", \"col4\": \"6/7/2011\", \"col5\": \"279\", \"col6\": \"1045\", \"col7\": \"20565.6206\", \"col8\": \"1971.5149\", \"col9\": \"616.0984\", \"col10\": \"23153.2339\", \"col11\": \"777\", \"col12\": \"3\", \"col13\": \"2024.994\", \"col14\": \"0\", \"col15\": \"6074.982\"}\n",
      "{\"col0\": \"43659\", \"col1\": \"3\", \"col2\": \"5/31/2011\", \"col3\": \"6/12/2011\", \"col4\": \"6/7/2011\", \"col5\": \"279\", \"col6\": \"1045\", \"col7\": \"20565.6206\", \"col8\": \"1971.5149\", \"col9\": \"616.0984\", \"col10\": \"23153.2339\", \"col11\": \"778\", \"col12\": \"1\", \"col13\": \"2024.994\", \"col14\": \"0\", \"col15\": \"2024.994\"}\n",
      "{\"col0\": \"43659\", \"col1\": \"4\", \"col2\": \"5/31/2011\", \"col3\": \"6/12/2011\", \"col4\": \"6/7/2011\", \"col5\": \"279\", \"col6\": \"1045\", \"col7\": \"20565.6206\", \"col8\": \"1971.5149\", \"col9\": \"616.0984\", \"col10\": \"23153.2339\", \"col11\": \"771\", \"col12\": \"1\", \"col13\": \"2039.994\", \"col14\": \"0\", \"col15\": \"2039.994\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/pyspark/sql/dataframe.py:147: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
      "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test script has successfully ran\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "inputDF = glueContext.create_dynamic_frame_from_options(connection_type=\"s3\",\n",
    "                                                        connection_options = {\"paths\": [\"s3://ti-dev-terraform-12345/raw/orders/orders.csv\"]}, format = \"csv\")\n",
    "inputDF.show(5)\n",
    "df = inputDF.toDF()\n",
    "df_pd = df.toPandas()\n",
    "print('test script has successfully ran')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b5dbbfc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/31 17:11:47 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------+--------------------+\n",
      "|id |firstname |lastname   |fullname            |\n",
      "+---+----------+-----------+--------------------+\n",
      "|293|Catherine |Abel       |Catherine Abel      |\n",
      "|295|Kim       |Abercrombie|Kim Abercrombie     |\n",
      "|297|Humberto  |Acevedo    |Humberto Acevedo    |\n",
      "|291|Gustavo   |Achong     |Gustavo Achong      |\n",
      "|299|Pilar     |Ackerman   |Pilar Ackerman      |\n",
      "|305|Carla     |Adams      |Carla Adams         |\n",
      "|301|Frances   |Adams      |Frances Adams       |\n",
      "|307|Jay       |Adams      |Jay Adams           |\n",
      "|309|Ronald    |Adina      |Ronald Adina        |\n",
      "|311|Samuel    |Agcaoili   |Samuel Agcaoili     |\n",
      "|313|James     |Aguilar    |James Aguilar       |\n",
      "|315|Robert    |Ahlering   |Robert Ahlering     |\n",
      "|319|Kim       |Akers      |Kim Akers           |\n",
      "|441|Stanley   |Alan       |Stanley Alan        |\n",
      "|323|Amy       |Alberts    |Amy Alberts         |\n",
      "|325|Anna      |Albright   |Anna Albright       |\n",
      "|327|Milton    |Albury     |Milton Albury       |\n",
      "|329|Paul      |Alcorn     |Paul Alcorn         |\n",
      "|331|Gregory   |Alderson   |Gregory Alderson    |\n",
      "|333|J. Phillip|Alexander  |J. Phillip Alexander|\n",
      "+---+----------+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- fullname: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark session (Glue image already configures this)\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"ReadCustomersFromS3\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# S3 path\n",
    "s3_path = \"s3://ti-unique-bucket-12345/customers/\"\n",
    "\n",
    "# Read CSV from S3\n",
    "df = (\n",
    "    spark.read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .csv(s3_path)\n",
    ")\n",
    "\n",
    "# Show data\n",
    "df.show(truncate=False)\n",
    "df.printSchema()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "177a2639",
   "metadata": {},
   "outputs": [],
   "source": [
    "from awsglue.dynamicframe import DynamicFrame\n",
    "\n",
    "dyf = DynamicFrame.fromDF(\n",
    "    df,\n",
    "    glue_context,\n",
    "    \"customers_dyf\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5dc016bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "database = 'dev_staging_test'\n",
    "\n",
    "staging_path = \"s3://ti-glue-catalog-bucket-12345/staging/\"\n",
    "project_folder = \"customers/\"\n",
    "target_path = staging_path+project_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849290b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from awsglue.context import GlueContext\n",
    "from awsglue.dynamicframe import DynamicFrame\n",
    "from pyspark.context import SparkContext\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "glue_context = GlueContext(sc)\n",
    "\n",
    "# Suppose df is your Spark DataFrame\n",
    "dyf = DynamicFrame.fromDF(df, glue_context, \"dyf\")\n",
    "\n",
    "glue_context.write_dynamic_frame.from_options(\n",
    "    frame = dyf,\n",
    "    connection_type = \"s3\",\n",
    "    connection_options = {\n",
    "        \"path\": \"s3://ti-glue-catalog-bucket-12345/staging/customers/\",\n",
    "        \"partitionKeys\": []   # Add if you want partitions\n",
    "    },\n",
    "    format = \"parquet\",\n",
    "    format_options = {\"compression\": \"snappy\"},\n",
    "    transformation_ctx = \"s3_write\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d38f182f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\": 293, \"firstname\": \"Catherine\", \"lastname\": \"Abel\", \"fullname\": \"Catherine Abel\"}\n",
      "{\"id\": 295, \"firstname\": \"Kim\", \"lastname\": \"Abercrombie\", \"fullname\": \"Kim Abercrombie\"}\n",
      "{\"id\": 297, \"firstname\": \"Humberto\", \"lastname\": \"Acevedo\", \"fullname\": \"Humberto Acevedo\"}\n",
      "{\"id\": 291, \"firstname\": \"Gustavo\", \"lastname\": \"Achong\", \"fullname\": \"Gustavo Achong\"}\n",
      "{\"id\": 299, \"firstname\": \"Pilar\", \"lastname\": \"Ackerman\", \"fullname\": \"Pilar Ackerman\"}\n",
      "{\"id\": 305, \"firstname\": \"Carla\", \"lastname\": \"Adams\", \"fullname\": \"Carla Adams\"}\n",
      "{\"id\": 301, \"firstname\": \"Frances\", \"lastname\": \"Adams\", \"fullname\": \"Frances Adams\"}\n",
      "{\"id\": 307, \"firstname\": \"Jay\", \"lastname\": \"Adams\", \"fullname\": \"Jay Adams\"}\n",
      "{\"id\": 309, \"firstname\": \"Ronald\", \"lastname\": \"Adina\", \"fullname\": \"Ronald Adina\"}\n",
      "{\"id\": 311, \"firstname\": \"Samuel\", \"lastname\": \"Agcaoili\", \"fullname\": \"Samuel Agcaoili\"}\n",
      "{\"id\": 313, \"firstname\": \"James\", \"lastname\": \"Aguilar\", \"fullname\": \"James Aguilar\"}\n",
      "{\"id\": 315, \"firstname\": \"Robert\", \"lastname\": \"Ahlering\", \"fullname\": \"Robert Ahlering\"}\n",
      "{\"id\": 319, \"firstname\": \"Kim\", \"lastname\": \"Akers\", \"fullname\": \"Kim Akers\"}\n",
      "{\"id\": 441, \"firstname\": \"Stanley\", \"lastname\": \"Alan\", \"fullname\": \"Stanley Alan\"}\n",
      "{\"id\": 323, \"firstname\": \"Amy\", \"lastname\": \"Alberts\", \"fullname\": \"Amy Alberts\"}\n",
      "{\"id\": 325, \"firstname\": \"Anna\", \"lastname\": \"Albright\", \"fullname\": \"Anna Albright\"}\n",
      "{\"id\": 327, \"firstname\": \"Milton\", \"lastname\": \"Albury\", \"fullname\": \"Milton Albury\"}\n",
      "{\"id\": 329, \"firstname\": \"Paul\", \"lastname\": \"Alcorn\", \"fullname\": \"Paul Alcorn\"}\n",
      "{\"id\": 331, \"firstname\": \"Gregory\", \"lastname\": \"Alderson\", \"fullname\": \"Gregory Alderson\"}\n",
      "{\"id\": 333, \"firstname\": \"J. Phillip\", \"lastname\": \"Alexander\", \"fullname\": \"J. Phillip Alexander\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dyf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "023457cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(F\"DROP TABLE {database}.customers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "cfa632b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"tmpdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c20bdad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE {database}.customers\n",
    "    USING parquet\n",
    "    LOCATION 's3://ti-glue-catalog-bucket-12345/staging/customers/'\n",
    "    AS\n",
    "    SELECT *\n",
    "    FROM tmpdf\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f74358",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
